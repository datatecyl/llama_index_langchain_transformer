{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OW6wXxld3BW"
      },
      "outputs": [],
      "source": [
        "# Mount google driver\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change to google driver folder which contains datasets\n",
        "# This folder will also be used to save model\n",
        "%cd /content/drive/MyDrive/Langchain_Llama2_Lab"
      ],
      "metadata": {
        "id": "XCF4DvqoeJGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install python packages\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "O1ely0bIeKv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from llama_index.schema import TextNode\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "248JXB8FeMoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define environment variable, path of data, model name and device\n",
        "os.environ[\"HF_HOME\"] = \"/content/huggingface\"  # Replace with your desired directory\n",
        "print(\"Please replace the text with your hugging face access token:\")\n",
        "os.environ[\"HF_HOME_TOKEN\"] = \"PLEASE_REPLACE_IT_WITH_YOUR_HF_ACCESS_TOKEN\"\n",
        "\n",
        "result_dir = '/content/drive/MyDrive/Langchain_Llama2_Lab/results'\n",
        "data_folder_path = '/content/drive/MyDrive/Langchain_Llama2_Lab/data/'\n",
        "vectorstore_path = '/content/drive/MyDrive/Langchain_Llama2_Lab/vectorstore/db_faiss/'\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "mCdzcyZRflF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Document:\n",
        "    def __init__(self, text, id):\n",
        "        self.text = text\n",
        "        self.id = id"
      ],
      "metadata": {
        "id": "zvh3cnUj7Uz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_documents():\n",
        "    loader = DirectoryLoader(data_folder_path, glob=\"*.pdf\", loader_cls=PyPDFLoader, show_progress=False)\n",
        "    documents = loader.load()\n",
        "    return documents"
      ],
      "metadata": {
        "id": "pxptMc9U07uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vectorstore():\n",
        "    # Load PDF files from data directory\n",
        "    documents = get_documents()\n",
        "    len(documents)\n",
        "\n",
        "    # Split text from PDF into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=120, chunk_overlap=0)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Load embeddings model\n",
        "    embedding_function = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': device})\n",
        "    embedding_function.embed_query(texts[0].page_content)\n",
        "\n",
        "    # Build and persist FAISS vector store\n",
        "    vector_database = FAISS.from_documents(texts, embedding_function)\n",
        "\n",
        "    vector_database.save_local(vectorstore_path)\n",
        "    print(\"Vector store created in: \", vectorstore_path)"
      ],
      "metadata": {
        "id": "_6PwqSCZgJ6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vectorstore():\n",
        "    # Load embeddings model\n",
        "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': device})\n",
        "\n",
        "    # Load vector store\n",
        "    vectorstore = FAISS.load_local(vectorstore_path, embeddings)\n",
        "\n",
        "    return vectorstore"
      ],
      "metadata": {
        "id": "CXt7cPqGg2d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "build_vectorstore()"
      ],
      "metadata": {
        "id": "AL7mNAMllISW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_vectorstore = get_vectorstore()"
      ],
      "metadata": {
        "id": "AaqI9KRsy4bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generateResponseText(prompt):\n",
        "    response = \"\"\n",
        "    response_raw_texts = loaded_vectorstore.similarity_search(prompt, top_k=1)\n",
        "    for document in response_raw_texts:\n",
        "        response += document.page_content\n",
        "    return response"
      ],
      "metadata": {
        "id": "p_qXq91vhoYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt = \"What is the name of your company?\"\n",
        "#prompt = \"What is the product lines of your company?\"\n",
        "#prompt = \"What are your services?\"\n",
        "prompt = \"\"\n",
        "while True:\n",
        "  prompt = input(\"Enter your input (press Enter when done): \" + \" \" * 5)\n",
        "  print(generateResponseText(prompt))"
      ],
      "metadata": {
        "id": "gJRY5byChibg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import (\n",
        "    SimpleDirectoryReader,\n",
        "    LLMPredictor,\n",
        "    PromptHelper, GPTListIndex\n",
        ")\n",
        "from llama_index.llms import LlamaCPP\n",
        "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
        "from llama_index.schema import BaseNode, Document\n"
      ],
      "metadata": {
        "id": "B4RX_nB1y69V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_size = 1024\n",
        "num_output = 100\n",
        "max_chunk_overlap = 0\n",
        "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)"
      ],
      "metadata": {
        "id": "EkwHXw8dlrpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF\n",
        "llm = LlamaCPP(\n",
        "    model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\",\n",
        "    model_path=None,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    context_window=3900,\n",
        "    generate_kwargs={},\n",
        "    model_kwargs={\"n_gpu_layers\": 1},\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "ZOivlCLjzAha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_predictor = LLMPredictor(llm)"
      ],
      "metadata": {
        "id": "V_4JNCfEzDGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_id_to_documents(documents):\n",
        "    node_list = []\n",
        "    for i, document in enumerate(documents):\n",
        "        doc_page_content = document.page_content\n",
        "        node = TextNode(text=doc_page_content, id_=i)\n",
        "        node_list.append(node)\n",
        "    return node_list"
      ],
      "metadata": {
        "id": "aMrEaPST8-8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = add_id_to_documents(get_documents())\n",
        "print(docs)"
      ],
      "metadata": {
        "id": "nSKYkAvV6ck4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs:\n",
        "    print(doc)"
      ],
      "metadata": {
        "id": "Pemt1s6e6h4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = GPTListIndex(docs, llm_predictor=llm_predictor, prompt_helper=prompt_helper)"
      ],
      "metadata": {
        "id": "BIgTR5vK53uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = index.as_query_engine().query(\"What is your product lines?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "fAMK3D8G58WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sktRrFVNxmS2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}